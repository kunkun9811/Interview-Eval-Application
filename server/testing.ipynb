{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine reading and realize in the end this article is not what you are looking for. Feels bad. Don't fret, we got you covered. Use our \"TLDR Bot\" to help you figure out if a article is what you needed for.\n",
    "\n",
    "Utilize TF-IDF to rank sentences based on importance and extract \"useful\" text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Tom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Tom\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import math\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get text from file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['As I have mentioned on my previous post, I am going to implement TF-IDF of a text which is a biography of the Beatles.', 'Bag of Words is an effective model to demonstrate documents as numerical vectors, but it is not enough to go further than enumeration.', 'TF-IDF is a technique that measures how important a word in a given document.', 'TF (Term Frequency) measures the frequency of a word in a document.', 'IDF (Inverse Document Frequency) measures the rank of the specific word for its relevancy within the text.', 'Stop words which contain unnecessary information such as “a”, “into” and “and” carry less importance in spite of their occurrence.', 'Thus, the TF-IDF is the product of TF and IDF: In order to acquire good results with TF-IDF, a huge corpus is necessary.', 'In my example, I just used a small sized corpus.', 'Since I removed stop words, result was pleasant.', 'As my previous code piece, we start again by adding modules to use their methods.', 'In this example, we utilize Scikit-learn besides Numpy, Pandas and Regular Expression.', 'Scikit-learn is a free machine learning library for python.', 'We will utilize CountVectorizer to convert a collection of text documents to a matrix of token counts.', 'TfidfTransformers handles transformation of a count matrix to a normalized TF or TF-IDF representation.', 'Data is fetched from ‘beatles_biography’ file and we are parse the text in order to obtain sentences.', 'Regular expression helps separation of sentences using marks then sentences are enlisted under sentences object.', 'We use 4 parameters in CountVectorizer method.', 'First one is stop_words which removes words that occur a lot but do not contain necessary information.', '‘None’ can be given if we don’t want to remove any word or we can give a list to choose which words are going to be swept ourselves.', 'In Scikit-learn, English stop word list is provided built-in.', 'min_df parameter is a threshold value where we ignore terms that have a document frequency lower than min_df.', 'max_df is the contrast of min_df parameter.', 'If the document frequency of a word is more than max_df, we ignore it.', 'ngram_range(x,y) is the last parameter which defines the boundary of n values for different n-grams.', 'x is for the minimum n value, y represents the maximum n value for n-grams.', 'fit_transform returns transform version of sentences.', 'We transform a count matrix to a normalized TF or TF-IDF representation to measure weights.', 'As I mentioned above, the word which has the highest weight provides more information about the document.', 'At the end of the transformation, list is acquired which comprises terms and their ranks.', 'Finally, we can print top 10 words through the given document.', 'TF-IDF is a numerical statistic used in information retrieval and text mining.', 'I wanted share my experience on it.', 'You can find full code from my repository.', 'In our next article we are going to continue with implementing word2vec to make relationships between words.', 'To say me “hi” or ask me anything: e-mail: toprakucar@gmail.com linkedin: https://www.linkedin.com/in/ktoprakucar/ github: https://github.com/ktoprakucar Written by Written by']\n"
     ]
    }
   ],
   "source": [
    "with open(\"testarticle.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "text = sent_tokenize(text)\n",
    "print(text)\n",
    "total_documents = len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Frequency Matrix for words Per Sentence(or document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'As I have menti': {'mention': 1, 'previou': 1, 'post': 1, ',': 1, 'go': 1, 'implement': 1, 'tf-idf': 1, 'text': 1, 'biographi': 1, 'beatl': 1, '.': 1}, 'Bag of Words is': {'bag': 1, 'word': 1, 'effect': 1, 'model': 1, 'demonstr': 1, 'document': 1, 'numer': 1, 'vector': 1, ',': 1, 'enough': 1, 'go': 1, 'enumer': 1, '.': 1}, 'TF-IDF is a tec': {'tf-idf': 1, 'techniqu': 1, 'measur': 1, 'import': 1, 'word': 1, 'given': 1, 'document': 1, '.': 1}, 'TF (Term Freque': {'tf': 1, '(': 1, 'term': 1, 'frequenc': 2, ')': 1, 'measur': 1, 'word': 1, 'document': 1, '.': 1}, 'IDF (Inverse Do': {'idf': 1, '(': 1, 'invers': 1, 'document': 1, 'frequenc': 1, ')': 1, 'measur': 1, 'rank': 1, 'specif': 1, 'word': 1, 'relev': 1, 'within': 1, 'text': 1, '.': 1}, 'Stop words whic': {'stop': 1, 'word': 1, 'contain': 1, 'unnecessari': 1, 'inform': 1, '“': 3, '”': 3, ',': 1, 'carri': 1, 'less': 1, 'import': 1, 'spite': 1, 'occurr': 1, '.': 1}, 'Thus, the TF-ID': {'thu': 1, ',': 2, 'tf-idf': 2, 'product': 1, 'tf': 1, 'idf': 1, ':': 1, 'order': 1, 'acquir': 1, 'good': 1, 'result': 1, 'huge': 1, 'corpu': 1, 'necessari': 1, '.': 1}, 'In my example, ': {'exampl': 1, ',': 1, 'use': 1, 'small': 1, 'size': 1, 'corpu': 1, '.': 1}, 'Since I removed': {'sinc': 1, 'remov': 1, 'stop': 1, 'word': 1, ',': 1, 'result': 1, 'wa': 1, 'pleasant': 1, '.': 1}, 'As my previous ': {'previou': 1, 'code': 1, 'piec': 1, ',': 1, 'start': 1, 'ad': 1, 'modul': 1, 'use': 1, 'method': 1, '.': 1}, 'In this example': {'thi': 1, 'exampl': 1, ',': 2, 'util': 1, 'scikit-learn': 1, 'besid': 1, 'numpi': 1, 'panda': 1, 'regular': 1, 'express': 1, '.': 1}, 'Scikit-learn is': {'scikit-learn': 1, 'free': 1, 'machin': 1, 'learn': 1, 'librari': 1, 'python': 1, '.': 1}, 'We will utilize': {'util': 1, 'countvector': 1, 'convert': 1, 'collect': 1, 'text': 1, 'document': 1, 'matrix': 1, 'token': 1, 'count': 1, '.': 1}, 'TfidfTransforme': {'tfidftransform': 1, 'handl': 1, 'transform': 1, 'count': 1, 'matrix': 1, 'normal': 1, 'tf': 1, 'tf-idf': 1, 'represent': 1, '.': 1}, 'Data is fetched': {'data': 1, 'fetch': 1, '‘': 1, 'beatles_biographi': 1, '’': 1, 'file': 1, 'pars': 1, 'text': 1, 'order': 1, 'obtain': 1, 'sentenc': 1, '.': 1}, 'Regular express': {'regular': 1, 'express': 1, 'help': 1, 'separ': 1, 'sentenc': 3, 'use': 1, 'mark': 1, 'enlist': 1, 'object': 1, '.': 1}, 'We use 4 parame': {'use': 1, '4': 1, 'paramet': 1, 'countvector': 1, 'method': 1, '.': 1}, 'First one is st': {'first': 1, 'one': 1, 'stop_word': 1, 'remov': 1, 'word': 1, 'occur': 1, 'lot': 1, 'contain': 1, 'necessari': 1, 'inform': 1, '.': 1}, '‘None’ can be g': {'‘': 1, 'none': 1, '’': 2, 'given': 1, 'want': 1, 'remov': 1, 'ani': 1, 'word': 2, 'give': 1, 'list': 1, 'choos': 1, 'go': 1, 'swept': 1, 'ourselv': 1, '.': 1}, 'In Scikit-learn': {'scikit-learn': 1, ',': 1, 'english': 1, 'stop': 1, 'word': 1, 'list': 1, 'provid': 1, 'built-in': 1, '.': 1}, 'min_df paramete': {'min_df': 2, 'paramet': 1, 'threshold': 1, 'valu': 1, 'ignor': 1, 'term': 1, 'document': 1, 'frequenc': 1, 'lower': 1, '.': 1}, 'max_df is the c': {'max_df': 1, 'contrast': 1, 'min_df': 1, 'paramet': 1, '.': 1}, 'If the document': {'document': 1, 'frequenc': 1, 'word': 1, 'max_df': 1, ',': 1, 'ignor': 1, '.': 1}, 'ngram_range(x,y': {'ngram_rang': 1, '(': 1, 'x': 1, ',': 1, ')': 1, 'last': 1, 'paramet': 1, 'defin': 1, 'boundari': 1, 'n': 1, 'valu': 1, 'differ': 1, 'n-gram': 1, '.': 1}, 'x is for the mi': {'x': 1, 'minimum': 1, 'n': 2, 'valu': 2, ',': 1, 'repres': 1, 'maximum': 1, 'n-gram': 1, '.': 1}, 'fit_transform r': {'fit_transform': 1, 'return': 1, 'transform': 1, 'version': 1, 'sentenc': 1, '.': 1}, 'We transform a ': {'transform': 1, 'count': 1, 'matrix': 1, 'normal': 1, 'tf': 1, 'tf-idf': 1, 'represent': 1, 'measur': 1, 'weight': 1, '.': 1}, 'As I mentioned ': {'mention': 1, 'abov': 1, ',': 1, 'word': 1, 'ha': 1, 'highest': 1, 'weight': 1, 'provid': 1, 'inform': 1, 'document': 1, '.': 1}, 'At the end of t': {'end': 1, 'transform': 1, ',': 1, 'list': 1, 'acquir': 1, 'compris': 1, 'term': 1, 'rank': 1, '.': 1}, 'Finally, we can': {'final': 1, ',': 1, 'print': 1, 'top': 1, '10': 1, 'word': 1, 'given': 1, 'document': 1, '.': 1}, 'TF-IDF is a num': {'tf-idf': 1, 'numer': 1, 'statist': 1, 'use': 1, 'inform': 1, 'retriev': 1, 'text': 1, 'mine': 1, '.': 1}, 'I wanted share ': {'want': 1, 'share': 1, 'experi': 1, '.': 1}, 'You can find fu': {'find': 1, 'full': 1, 'code': 1, 'repositori': 1, '.': 1}, 'In our next art': {'next': 1, 'articl': 1, 'go': 1, 'continu': 1, 'implement': 1, 'word2vec': 1, 'make': 1, 'relationship': 1, 'word': 1, '.': 1}, 'To say me “hi” ': {'say': 1, '“': 1, 'hi': 1, '”': 1, 'ask': 1, 'anyth': 1, ':': 6, 'e-mail': 1, 'toprakucar': 1, '@': 1, 'gmail.com': 1, 'linkedin': 1, 'http': 2, '//www.linkedin.com/in/ktoprakucar/': 1, 'github': 1, '//github.com/ktoprakucar': 1, 'written': 2}}\n"
     ]
    }
   ],
   "source": [
    "def create_frequency_matrix(sentences) -> dict:\n",
    "    # set stopwords and stemmer\n",
    "    stopWords = set(stopwords.words(\"english\"))\n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    #initialize freqency matrix\n",
    "    frequency_matrix = {}\n",
    "    keys = []\n",
    "    value_table = []\n",
    "    \n",
    "    # get first 15 chars per sentence as key for frequency matrix\n",
    "    keys = list(map(lambda x: x[:15], sentences))\n",
    "    #print(keys)\n",
    "    \n",
    "    #create list of dicts for word frequency for each sentence\n",
    "    for sentence in sentences:\n",
    "        freq_table = {}\n",
    "        # tokenize words: SPLITTING\n",
    "        tokenized = word_tokenize(sentence)\n",
    "        # clean words: REDUCING\n",
    "        for word in tokenized:\n",
    "            word = word.lower()\n",
    "            word = ps.stem(word)\n",
    "            if word in stopWords:\n",
    "                continue\n",
    "            if word in freq_table:\n",
    "                freq_table[word] += 1\n",
    "            else:\n",
    "                freq_table[word] = 1\n",
    "\n",
    "        value_table.append(freq_table)\n",
    "        #print(freq_table)\n",
    "    return dict(zip(keys, value_table))\n",
    "\n",
    "freq_matrix = create_frequency_matrix(text)\n",
    "print(freq_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create TF Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'As I have menti': {'mention': 0.09090909090909091, 'previou': 0.09090909090909091, 'post': 0.09090909090909091, ',': 0.09090909090909091, 'go': 0.09090909090909091, 'implement': 0.09090909090909091, 'tf-idf': 0.09090909090909091, 'text': 0.09090909090909091, 'biographi': 0.09090909090909091, 'beatl': 0.09090909090909091, '.': 0.09090909090909091}, 'Bag of Words is': {'bag': 0.07692307692307693, 'word': 0.07692307692307693, 'effect': 0.07692307692307693, 'model': 0.07692307692307693, 'demonstr': 0.07692307692307693, 'document': 0.07692307692307693, 'numer': 0.07692307692307693, 'vector': 0.07692307692307693, ',': 0.07692307692307693, 'enough': 0.07692307692307693, 'go': 0.07692307692307693, 'enumer': 0.07692307692307693, '.': 0.07692307692307693}, 'TF-IDF is a tec': {'tf-idf': 0.125, 'techniqu': 0.125, 'measur': 0.125, 'import': 0.125, 'word': 0.125, 'given': 0.125, 'document': 0.125, '.': 0.125}, 'TF (Term Freque': {'tf': 0.1, '(': 0.1, 'term': 0.1, 'frequenc': 0.2, ')': 0.1, 'measur': 0.1, 'word': 0.1, 'document': 0.1, '.': 0.1}, 'IDF (Inverse Do': {'idf': 0.07142857142857142, '(': 0.07142857142857142, 'invers': 0.07142857142857142, 'document': 0.07142857142857142, 'frequenc': 0.07142857142857142, ')': 0.07142857142857142, 'measur': 0.07142857142857142, 'rank': 0.07142857142857142, 'specif': 0.07142857142857142, 'word': 0.07142857142857142, 'relev': 0.07142857142857142, 'within': 0.07142857142857142, 'text': 0.07142857142857142, '.': 0.07142857142857142}, 'Stop words whic': {'stop': 0.05555555555555555, 'word': 0.05555555555555555, 'contain': 0.05555555555555555, 'unnecessari': 0.05555555555555555, 'inform': 0.05555555555555555, '“': 0.16666666666666666, '”': 0.16666666666666666, ',': 0.05555555555555555, 'carri': 0.05555555555555555, 'less': 0.05555555555555555, 'import': 0.05555555555555555, 'spite': 0.05555555555555555, 'occurr': 0.05555555555555555, '.': 0.05555555555555555}, 'Thus, the TF-ID': {'thu': 0.058823529411764705, ',': 0.11764705882352941, 'tf-idf': 0.11764705882352941, 'product': 0.058823529411764705, 'tf': 0.058823529411764705, 'idf': 0.058823529411764705, ':': 0.058823529411764705, 'order': 0.058823529411764705, 'acquir': 0.058823529411764705, 'good': 0.058823529411764705, 'result': 0.058823529411764705, 'huge': 0.058823529411764705, 'corpu': 0.058823529411764705, 'necessari': 0.058823529411764705, '.': 0.058823529411764705}, 'In my example, ': {'exampl': 0.14285714285714285, ',': 0.14285714285714285, 'use': 0.14285714285714285, 'small': 0.14285714285714285, 'size': 0.14285714285714285, 'corpu': 0.14285714285714285, '.': 0.14285714285714285}, 'Since I removed': {'sinc': 0.1111111111111111, 'remov': 0.1111111111111111, 'stop': 0.1111111111111111, 'word': 0.1111111111111111, ',': 0.1111111111111111, 'result': 0.1111111111111111, 'wa': 0.1111111111111111, 'pleasant': 0.1111111111111111, '.': 0.1111111111111111}, 'As my previous ': {'previou': 0.1, 'code': 0.1, 'piec': 0.1, ',': 0.1, 'start': 0.1, 'ad': 0.1, 'modul': 0.1, 'use': 0.1, 'method': 0.1, '.': 0.1}, 'In this example': {'thi': 0.08333333333333333, 'exampl': 0.08333333333333333, ',': 0.16666666666666666, 'util': 0.08333333333333333, 'scikit-learn': 0.08333333333333333, 'besid': 0.08333333333333333, 'numpi': 0.08333333333333333, 'panda': 0.08333333333333333, 'regular': 0.08333333333333333, 'express': 0.08333333333333333, '.': 0.08333333333333333}, 'Scikit-learn is': {'scikit-learn': 0.14285714285714285, 'free': 0.14285714285714285, 'machin': 0.14285714285714285, 'learn': 0.14285714285714285, 'librari': 0.14285714285714285, 'python': 0.14285714285714285, '.': 0.14285714285714285}, 'We will utilize': {'util': 0.1, 'countvector': 0.1, 'convert': 0.1, 'collect': 0.1, 'text': 0.1, 'document': 0.1, 'matrix': 0.1, 'token': 0.1, 'count': 0.1, '.': 0.1}, 'TfidfTransforme': {'tfidftransform': 0.1, 'handl': 0.1, 'transform': 0.1, 'count': 0.1, 'matrix': 0.1, 'normal': 0.1, 'tf': 0.1, 'tf-idf': 0.1, 'represent': 0.1, '.': 0.1}, 'Data is fetched': {'data': 0.08333333333333333, 'fetch': 0.08333333333333333, '‘': 0.08333333333333333, 'beatles_biographi': 0.08333333333333333, '’': 0.08333333333333333, 'file': 0.08333333333333333, 'pars': 0.08333333333333333, 'text': 0.08333333333333333, 'order': 0.08333333333333333, 'obtain': 0.08333333333333333, 'sentenc': 0.08333333333333333, '.': 0.08333333333333333}, 'Regular express': {'regular': 0.08333333333333333, 'express': 0.08333333333333333, 'help': 0.08333333333333333, 'separ': 0.08333333333333333, 'sentenc': 0.25, 'use': 0.08333333333333333, 'mark': 0.08333333333333333, 'enlist': 0.08333333333333333, 'object': 0.08333333333333333, '.': 0.08333333333333333}, 'We use 4 parame': {'use': 0.16666666666666666, '4': 0.16666666666666666, 'paramet': 0.16666666666666666, 'countvector': 0.16666666666666666, 'method': 0.16666666666666666, '.': 0.16666666666666666}, 'First one is st': {'first': 0.09090909090909091, 'one': 0.09090909090909091, 'stop_word': 0.09090909090909091, 'remov': 0.09090909090909091, 'word': 0.09090909090909091, 'occur': 0.09090909090909091, 'lot': 0.09090909090909091, 'contain': 0.09090909090909091, 'necessari': 0.09090909090909091, 'inform': 0.09090909090909091, '.': 0.09090909090909091}, '‘None’ can be g': {'‘': 0.058823529411764705, 'none': 0.058823529411764705, '’': 0.11764705882352941, 'given': 0.058823529411764705, 'want': 0.058823529411764705, 'remov': 0.058823529411764705, 'ani': 0.058823529411764705, 'word': 0.11764705882352941, 'give': 0.058823529411764705, 'list': 0.058823529411764705, 'choos': 0.058823529411764705, 'go': 0.058823529411764705, 'swept': 0.058823529411764705, 'ourselv': 0.058823529411764705, '.': 0.058823529411764705}, 'In Scikit-learn': {'scikit-learn': 0.1111111111111111, ',': 0.1111111111111111, 'english': 0.1111111111111111, 'stop': 0.1111111111111111, 'word': 0.1111111111111111, 'list': 0.1111111111111111, 'provid': 0.1111111111111111, 'built-in': 0.1111111111111111, '.': 0.1111111111111111}, 'min_df paramete': {'min_df': 0.18181818181818182, 'paramet': 0.09090909090909091, 'threshold': 0.09090909090909091, 'valu': 0.09090909090909091, 'ignor': 0.09090909090909091, 'term': 0.09090909090909091, 'document': 0.09090909090909091, 'frequenc': 0.09090909090909091, 'lower': 0.09090909090909091, '.': 0.09090909090909091}, 'max_df is the c': {'max_df': 0.2, 'contrast': 0.2, 'min_df': 0.2, 'paramet': 0.2, '.': 0.2}, 'If the document': {'document': 0.14285714285714285, 'frequenc': 0.14285714285714285, 'word': 0.14285714285714285, 'max_df': 0.14285714285714285, ',': 0.14285714285714285, 'ignor': 0.14285714285714285, '.': 0.14285714285714285}, 'ngram_range(x,y': {'ngram_rang': 0.07142857142857142, '(': 0.07142857142857142, 'x': 0.07142857142857142, ',': 0.07142857142857142, ')': 0.07142857142857142, 'last': 0.07142857142857142, 'paramet': 0.07142857142857142, 'defin': 0.07142857142857142, 'boundari': 0.07142857142857142, 'n': 0.07142857142857142, 'valu': 0.07142857142857142, 'differ': 0.07142857142857142, 'n-gram': 0.07142857142857142, '.': 0.07142857142857142}, 'x is for the mi': {'x': 0.09090909090909091, 'minimum': 0.09090909090909091, 'n': 0.18181818181818182, 'valu': 0.18181818181818182, ',': 0.09090909090909091, 'repres': 0.09090909090909091, 'maximum': 0.09090909090909091, 'n-gram': 0.09090909090909091, '.': 0.09090909090909091}, 'fit_transform r': {'fit_transform': 0.16666666666666666, 'return': 0.16666666666666666, 'transform': 0.16666666666666666, 'version': 0.16666666666666666, 'sentenc': 0.16666666666666666, '.': 0.16666666666666666}, 'We transform a ': {'transform': 0.1, 'count': 0.1, 'matrix': 0.1, 'normal': 0.1, 'tf': 0.1, 'tf-idf': 0.1, 'represent': 0.1, 'measur': 0.1, 'weight': 0.1, '.': 0.1}, 'As I mentioned ': {'mention': 0.09090909090909091, 'abov': 0.09090909090909091, ',': 0.09090909090909091, 'word': 0.09090909090909091, 'ha': 0.09090909090909091, 'highest': 0.09090909090909091, 'weight': 0.09090909090909091, 'provid': 0.09090909090909091, 'inform': 0.09090909090909091, 'document': 0.09090909090909091, '.': 0.09090909090909091}, 'At the end of t': {'end': 0.1111111111111111, 'transform': 0.1111111111111111, ',': 0.1111111111111111, 'list': 0.1111111111111111, 'acquir': 0.1111111111111111, 'compris': 0.1111111111111111, 'term': 0.1111111111111111, 'rank': 0.1111111111111111, '.': 0.1111111111111111}, 'Finally, we can': {'final': 0.1111111111111111, ',': 0.1111111111111111, 'print': 0.1111111111111111, 'top': 0.1111111111111111, '10': 0.1111111111111111, 'word': 0.1111111111111111, 'given': 0.1111111111111111, 'document': 0.1111111111111111, '.': 0.1111111111111111}, 'TF-IDF is a num': {'tf-idf': 0.1111111111111111, 'numer': 0.1111111111111111, 'statist': 0.1111111111111111, 'use': 0.1111111111111111, 'inform': 0.1111111111111111, 'retriev': 0.1111111111111111, 'text': 0.1111111111111111, 'mine': 0.1111111111111111, '.': 0.1111111111111111}, 'I wanted share ': {'want': 0.25, 'share': 0.25, 'experi': 0.25, '.': 0.25}, 'You can find fu': {'find': 0.2, 'full': 0.2, 'code': 0.2, 'repositori': 0.2, '.': 0.2}, 'In our next art': {'next': 0.1, 'articl': 0.1, 'go': 0.1, 'continu': 0.1, 'implement': 0.1, 'word2vec': 0.1, 'make': 0.1, 'relationship': 0.1, 'word': 0.1, '.': 0.1}, 'To say me “hi” ': {'say': 0.041666666666666664, '“': 0.041666666666666664, 'hi': 0.041666666666666664, '”': 0.041666666666666664, 'ask': 0.041666666666666664, 'anyth': 0.041666666666666664, ':': 0.25, 'e-mail': 0.041666666666666664, 'toprakucar': 0.041666666666666664, '@': 0.041666666666666664, 'gmail.com': 0.041666666666666664, 'linkedin': 0.041666666666666664, 'http': 0.08333333333333333, '//www.linkedin.com/in/ktoprakucar/': 0.041666666666666664, 'github': 0.041666666666666664, '//github.com/ktoprakucar': 0.041666666666666664, 'written': 0.08333333333333333}}\n"
     ]
    }
   ],
   "source": [
    "def create_tf_matrix(freq_matrix):\n",
    "    tf_matrix = {}\n",
    "    \n",
    "    for sentence, freq_table in freq_matrix.items():\n",
    "        tf_table = {}\n",
    "        \n",
    "#         diff_words_in_sentence = len(freq_table)\n",
    "        diff_words_in_sentence = sum([freq_table[t] for t in freq_table])\n",
    "        \n",
    "        for word, count in freq_table.items():\n",
    "            tf_table[word] = count / diff_words_in_sentence\n",
    "            \n",
    "        tf_matrix[sentence] = tf_table\n",
    "        \n",
    "    return tf_matrix\n",
    "\n",
    "tf_matrix = create_tf_matrix(freq_matrix)\n",
    "print(tf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a table for documents per words \n",
    "### For each word, count how many times it appears in the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mention': 2, 'previou': 2, 'post': 1, ',': 15, 'go': 4, 'implement': 2, 'tf-idf': 6, 'text': 5, 'biographi': 1, 'beatl': 1, '.': 34, 'bag': 1, 'word': 13, 'effect': 1, 'model': 1, 'demonstr': 1, 'document': 9, 'numer': 2, 'vector': 1, 'enough': 1, 'enumer': 1, 'techniqu': 1, 'measur': 4, 'import': 2, 'given': 3, 'tf': 4, '(': 3, 'term': 3, 'frequenc': 4, ')': 3, 'idf': 2, 'invers': 1, 'rank': 2, 'specif': 1, 'relev': 1, 'within': 1, 'stop': 3, 'contain': 2, 'unnecessari': 1, 'inform': 4, '“': 2, '”': 2, 'carri': 1, 'less': 1, 'spite': 1, 'occurr': 1, 'thu': 1, 'product': 1, ':': 2, 'order': 2, 'acquir': 2, 'good': 1, 'result': 2, 'huge': 1, 'corpu': 2, 'necessari': 2, 'exampl': 2, 'use': 5, 'small': 1, 'size': 1, 'sinc': 1, 'remov': 3, 'wa': 1, 'pleasant': 1, 'code': 2, 'piec': 1, 'start': 1, 'ad': 1, 'modul': 1, 'method': 2, 'thi': 1, 'util': 2, 'scikit-learn': 3, 'besid': 1, 'numpi': 1, 'panda': 1, 'regular': 2, 'express': 2, 'free': 1, 'machin': 1, 'learn': 1, 'librari': 1, 'python': 1, 'countvector': 2, 'convert': 1, 'collect': 1, 'matrix': 3, 'token': 1, 'count': 3, 'tfidftransform': 1, 'handl': 1, 'transform': 4, 'normal': 2, 'represent': 2, 'data': 1, 'fetch': 1, '‘': 2, 'beatles_biographi': 1, '’': 2, 'file': 1, 'pars': 1, 'obtain': 1, 'sentenc': 3, 'help': 1, 'separ': 1, 'mark': 1, 'enlist': 1, 'object': 1, '4': 1, 'paramet': 4, 'first': 1, 'one': 1, 'stop_word': 1, 'occur': 1, 'lot': 1, 'none': 1, 'want': 2, 'ani': 1, 'give': 1, 'list': 3, 'choos': 1, 'swept': 1, 'ourselv': 1, 'english': 1, 'provid': 2, 'built-in': 1, 'min_df': 2, 'threshold': 1, 'valu': 3, 'ignor': 2, 'lower': 1, 'max_df': 2, 'contrast': 1, 'ngram_rang': 1, 'x': 2, 'last': 1, 'defin': 1, 'boundari': 1, 'n': 2, 'differ': 1, 'n-gram': 2, 'minimum': 1, 'repres': 1, 'maximum': 1, 'fit_transform': 1, 'return': 1, 'version': 1, 'weight': 2, 'abov': 1, 'ha': 1, 'highest': 1, 'end': 1, 'compris': 1, 'final': 1, 'print': 1, 'top': 1, '10': 1, 'statist': 1, 'retriev': 1, 'mine': 1, 'share': 1, 'experi': 1, 'find': 1, 'full': 1, 'repositori': 1, 'next': 1, 'articl': 1, 'continu': 1, 'word2vec': 1, 'make': 1, 'relationship': 1, 'say': 1, 'hi': 1, 'ask': 1, 'anyth': 1, 'e-mail': 1, 'toprakucar': 1, '@': 1, 'gmail.com': 1, 'linkedin': 1, 'http': 1, '//www.linkedin.com/in/ktoprakucar/': 1, 'github': 1, '//github.com/ktoprakucar': 1, 'written': 1}\n"
     ]
    }
   ],
   "source": [
    "#calculate document frequency\n",
    "def create_documents_per_words(freq_matrix):\n",
    "    doc_per_word_table = {}\n",
    "    \n",
    "    for _, word_freq in freq_matrix.items():\n",
    "        for word, freq in word_freq.items():\n",
    "            if word in doc_per_word_table:\n",
    "                doc_per_word_table[word] += 1\n",
    "            else:\n",
    "                doc_per_word_table[word] = 1\n",
    "    \n",
    "    return doc_per_word_table\n",
    "\n",
    "doc_per_word = create_documents_per_words(freq_matrix)\n",
    "print(doc_per_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create IDF matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'As I have menti': {'mention': 1.2430380486862944, 'previou': 1.2430380486862944, 'post': 1.5440680443502757, ',': 0.36797678529459443, 'go': 0.9420080530223133, 'implement': 1.2430380486862944, 'tf-idf': 0.7659167939666319, 'text': 0.8450980400142568, 'biographi': 1.5440680443502757, 'beatl': 1.5440680443502757, '.': 0.012589127308020467}, 'Bag of Words is': {'bag': 1.5440680443502757, 'word': 0.4301246920434389, 'effect': 1.5440680443502757, 'model': 1.5440680443502757, 'demonstr': 1.5440680443502757, 'document': 0.5898255349109508, 'numer': 1.2430380486862944, 'vector': 1.5440680443502757, ',': 0.36797678529459443, 'enough': 1.5440680443502757, 'go': 0.9420080530223133, 'enumer': 1.5440680443502757, '.': 0.012589127308020467}, 'TF-IDF is a tec': {'tf-idf': 0.7659167939666319, 'techniqu': 1.5440680443502757, 'measur': 0.9420080530223133, 'import': 1.2430380486862944, 'word': 0.4301246920434389, 'given': 1.066946789630613, 'document': 0.5898255349109508, '.': 0.012589127308020467}, 'TF (Term Freque': {'tf': 0.9420080530223133, '(': 1.066946789630613, 'term': 1.066946789630613, 'frequenc': 0.9420080530223133, ')': 1.066946789630613, 'measur': 0.9420080530223133, 'word': 0.4301246920434389, 'document': 0.5898255349109508, '.': 0.012589127308020467}, 'IDF (Inverse Do': {'idf': 1.2430380486862944, '(': 1.066946789630613, 'invers': 1.5440680443502757, 'document': 0.5898255349109508, 'frequenc': 0.9420080530223133, ')': 1.066946789630613, 'measur': 0.9420080530223133, 'rank': 1.2430380486862944, 'specif': 1.5440680443502757, 'word': 0.4301246920434389, 'relev': 1.5440680443502757, 'within': 1.5440680443502757, 'text': 0.8450980400142568, '.': 0.012589127308020467}, 'Stop words whic': {'stop': 1.066946789630613, 'word': 0.4301246920434389, 'contain': 1.2430380486862944, 'unnecessari': 1.5440680443502757, 'inform': 0.9420080530223133, '“': 1.2430380486862944, '”': 1.2430380486862944, ',': 0.36797678529459443, 'carri': 1.5440680443502757, 'less': 1.5440680443502757, 'import': 1.2430380486862944, 'spite': 1.5440680443502757, 'occurr': 1.5440680443502757, '.': 0.012589127308020467}, 'Thus, the TF-ID': {'thu': 1.5440680443502757, ',': 0.36797678529459443, 'tf-idf': 0.7659167939666319, 'product': 1.5440680443502757, 'tf': 0.9420080530223133, 'idf': 1.2430380486862944, ':': 1.2430380486862944, 'order': 1.2430380486862944, 'acquir': 1.2430380486862944, 'good': 1.5440680443502757, 'result': 1.2430380486862944, 'huge': 1.5440680443502757, 'corpu': 1.2430380486862944, 'necessari': 1.2430380486862944, '.': 0.012589127308020467}, 'In my example, ': {'exampl': 1.2430380486862944, ',': 0.36797678529459443, 'use': 0.8450980400142568, 'small': 1.5440680443502757, 'size': 1.5440680443502757, 'corpu': 1.2430380486862944, '.': 0.012589127308020467}, 'Since I removed': {'sinc': 1.5440680443502757, 'remov': 1.066946789630613, 'stop': 1.066946789630613, 'word': 0.4301246920434389, ',': 0.36797678529459443, 'result': 1.2430380486862944, 'wa': 1.5440680443502757, 'pleasant': 1.5440680443502757, '.': 0.012589127308020467}, 'As my previous ': {'previou': 1.2430380486862944, 'code': 1.2430380486862944, 'piec': 1.5440680443502757, ',': 0.36797678529459443, 'start': 1.5440680443502757, 'ad': 1.5440680443502757, 'modul': 1.5440680443502757, 'use': 0.8450980400142568, 'method': 1.2430380486862944, '.': 0.012589127308020467}, 'In this example': {'thi': 1.5440680443502757, 'exampl': 1.2430380486862944, ',': 0.36797678529459443, 'util': 1.2430380486862944, 'scikit-learn': 1.066946789630613, 'besid': 1.5440680443502757, 'numpi': 1.5440680443502757, 'panda': 1.5440680443502757, 'regular': 1.2430380486862944, 'express': 1.2430380486862944, '.': 0.012589127308020467}, 'Scikit-learn is': {'scikit-learn': 1.066946789630613, 'free': 1.5440680443502757, 'machin': 1.5440680443502757, 'learn': 1.5440680443502757, 'librari': 1.5440680443502757, 'python': 1.5440680443502757, '.': 0.012589127308020467}, 'We will utilize': {'util': 1.2430380486862944, 'countvector': 1.2430380486862944, 'convert': 1.5440680443502757, 'collect': 1.5440680443502757, 'text': 0.8450980400142568, 'document': 0.5898255349109508, 'matrix': 1.066946789630613, 'token': 1.5440680443502757, 'count': 1.066946789630613, '.': 0.012589127308020467}, 'TfidfTransforme': {'tfidftransform': 1.5440680443502757, 'handl': 1.5440680443502757, 'transform': 0.9420080530223133, 'count': 1.066946789630613, 'matrix': 1.066946789630613, 'normal': 1.2430380486862944, 'tf': 0.9420080530223133, 'tf-idf': 0.7659167939666319, 'represent': 1.2430380486862944, '.': 0.012589127308020467}, 'Data is fetched': {'data': 1.5440680443502757, 'fetch': 1.5440680443502757, '‘': 1.2430380486862944, 'beatles_biographi': 1.5440680443502757, '’': 1.2430380486862944, 'file': 1.5440680443502757, 'pars': 1.5440680443502757, 'text': 0.8450980400142568, 'order': 1.2430380486862944, 'obtain': 1.5440680443502757, 'sentenc': 1.066946789630613, '.': 0.012589127308020467}, 'Regular express': {'regular': 1.2430380486862944, 'express': 1.2430380486862944, 'help': 1.5440680443502757, 'separ': 1.5440680443502757, 'sentenc': 1.066946789630613, 'use': 0.8450980400142568, 'mark': 1.5440680443502757, 'enlist': 1.5440680443502757, 'object': 1.5440680443502757, '.': 0.012589127308020467}, 'We use 4 parame': {'use': 0.8450980400142568, '4': 1.5440680443502757, 'paramet': 0.9420080530223133, 'countvector': 1.2430380486862944, 'method': 1.2430380486862944, '.': 0.012589127308020467}, 'First one is st': {'first': 1.5440680443502757, 'one': 1.5440680443502757, 'stop_word': 1.5440680443502757, 'remov': 1.066946789630613, 'word': 0.4301246920434389, 'occur': 1.5440680443502757, 'lot': 1.5440680443502757, 'contain': 1.2430380486862944, 'necessari': 1.2430380486862944, 'inform': 0.9420080530223133, '.': 0.012589127308020467}, '‘None’ can be g': {'‘': 1.2430380486862944, 'none': 1.5440680443502757, '’': 1.2430380486862944, 'given': 1.066946789630613, 'want': 1.2430380486862944, 'remov': 1.066946789630613, 'ani': 1.5440680443502757, 'word': 0.4301246920434389, 'give': 1.5440680443502757, 'list': 1.066946789630613, 'choos': 1.5440680443502757, 'go': 0.9420080530223133, 'swept': 1.5440680443502757, 'ourselv': 1.5440680443502757, '.': 0.012589127308020467}, 'In Scikit-learn': {'scikit-learn': 1.066946789630613, ',': 0.36797678529459443, 'english': 1.5440680443502757, 'stop': 1.066946789630613, 'word': 0.4301246920434389, 'list': 1.066946789630613, 'provid': 1.2430380486862944, 'built-in': 1.5440680443502757, '.': 0.012589127308020467}, 'min_df paramete': {'min_df': 1.2430380486862944, 'paramet': 0.9420080530223133, 'threshold': 1.5440680443502757, 'valu': 1.066946789630613, 'ignor': 1.2430380486862944, 'term': 1.066946789630613, 'document': 0.5898255349109508, 'frequenc': 0.9420080530223133, 'lower': 1.5440680443502757, '.': 0.012589127308020467}, 'max_df is the c': {'max_df': 1.2430380486862944, 'contrast': 1.5440680443502757, 'min_df': 1.2430380486862944, 'paramet': 0.9420080530223133, '.': 0.012589127308020467}, 'If the document': {'document': 0.5898255349109508, 'frequenc': 0.9420080530223133, 'word': 0.4301246920434389, 'max_df': 1.2430380486862944, ',': 0.36797678529459443, 'ignor': 1.2430380486862944, '.': 0.012589127308020467}, 'ngram_range(x,y': {'ngram_rang': 1.5440680443502757, '(': 1.066946789630613, 'x': 1.2430380486862944, ',': 0.36797678529459443, ')': 1.066946789630613, 'last': 1.5440680443502757, 'paramet': 0.9420080530223133, 'defin': 1.5440680443502757, 'boundari': 1.5440680443502757, 'n': 1.2430380486862944, 'valu': 1.066946789630613, 'differ': 1.5440680443502757, 'n-gram': 1.2430380486862944, '.': 0.012589127308020467}, 'x is for the mi': {'x': 1.2430380486862944, 'minimum': 1.5440680443502757, 'n': 1.2430380486862944, 'valu': 1.066946789630613, ',': 0.36797678529459443, 'repres': 1.5440680443502757, 'maximum': 1.5440680443502757, 'n-gram': 1.2430380486862944, '.': 0.012589127308020467}, 'fit_transform r': {'fit_transform': 1.5440680443502757, 'return': 1.5440680443502757, 'transform': 0.9420080530223133, 'version': 1.5440680443502757, 'sentenc': 1.066946789630613, '.': 0.012589127308020467}, 'We transform a ': {'transform': 0.9420080530223133, 'count': 1.066946789630613, 'matrix': 1.066946789630613, 'normal': 1.2430380486862944, 'tf': 0.9420080530223133, 'tf-idf': 0.7659167939666319, 'represent': 1.2430380486862944, 'measur': 0.9420080530223133, 'weight': 1.2430380486862944, '.': 0.012589127308020467}, 'As I mentioned ': {'mention': 1.2430380486862944, 'abov': 1.5440680443502757, ',': 0.36797678529459443, 'word': 0.4301246920434389, 'ha': 1.5440680443502757, 'highest': 1.5440680443502757, 'weight': 1.2430380486862944, 'provid': 1.2430380486862944, 'inform': 0.9420080530223133, 'document': 0.5898255349109508, '.': 0.012589127308020467}, 'At the end of t': {'end': 1.5440680443502757, 'transform': 0.9420080530223133, ',': 0.36797678529459443, 'list': 1.066946789630613, 'acquir': 1.2430380486862944, 'compris': 1.5440680443502757, 'term': 1.066946789630613, 'rank': 1.2430380486862944, '.': 0.012589127308020467}, 'Finally, we can': {'final': 1.5440680443502757, ',': 0.36797678529459443, 'print': 1.5440680443502757, 'top': 1.5440680443502757, '10': 1.5440680443502757, 'word': 0.4301246920434389, 'given': 1.066946789630613, 'document': 0.5898255349109508, '.': 0.012589127308020467}, 'TF-IDF is a num': {'tf-idf': 0.7659167939666319, 'numer': 1.2430380486862944, 'statist': 1.5440680443502757, 'use': 0.8450980400142568, 'inform': 0.9420080530223133, 'retriev': 1.5440680443502757, 'text': 0.8450980400142568, 'mine': 1.5440680443502757, '.': 0.012589127308020467}, 'I wanted share ': {'want': 1.2430380486862944, 'share': 1.5440680443502757, 'experi': 1.5440680443502757, '.': 0.012589127308020467}, 'You can find fu': {'find': 1.5440680443502757, 'full': 1.5440680443502757, 'code': 1.2430380486862944, 'repositori': 1.5440680443502757, '.': 0.012589127308020467}, 'In our next art': {'next': 1.5440680443502757, 'articl': 1.5440680443502757, 'go': 0.9420080530223133, 'continu': 1.5440680443502757, 'implement': 1.2430380486862944, 'word2vec': 1.5440680443502757, 'make': 1.5440680443502757, 'relationship': 1.5440680443502757, 'word': 0.4301246920434389, '.': 0.012589127308020467}, 'To say me “hi” ': {'say': 1.5440680443502757, '“': 1.2430380486862944, 'hi': 1.5440680443502757, '”': 1.2430380486862944, 'ask': 1.5440680443502757, 'anyth': 1.5440680443502757, ':': 1.2430380486862944, 'e-mail': 1.5440680443502757, 'toprakucar': 1.5440680443502757, '@': 1.5440680443502757, 'gmail.com': 1.5440680443502757, 'linkedin': 1.5440680443502757, 'http': 1.5440680443502757, '//www.linkedin.com/in/ktoprakucar/': 1.5440680443502757, 'github': 1.5440680443502757, '//github.com/ktoprakucar': 1.5440680443502757, 'written': 1.5440680443502757}}\n"
     ]
    }
   ],
   "source": [
    "def create_idf_matrix(freq_matrix, doc_per_word_, total_documents):\n",
    "    idf_matrix = {}\n",
    "    \n",
    "    for sentence, freq_table in freq_matrix.items():\n",
    "        idf_table = {}\n",
    "        \n",
    "        for word in freq_table.keys():\n",
    "            idf_table[word] = math.log10(total_documents/ float(doc_per_word[word]))\n",
    "            \n",
    "        idf_matrix[sentence] = idf_table\n",
    "        \n",
    "    return idf_matrix\n",
    "\n",
    "idf_matrix = create_idf_matrix(freq_matrix, doc_per_word, total_documents)\n",
    "print(idf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate TF-IDF and generate a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'As I have menti': {'mention': 0.11300345897148131, 'previou': 0.11300345897148131, 'post': 0.14036982221366143, ',': 0.033452435026781316, 'go': 0.08563709572930121, 'implement': 0.11300345897148131, 'tf-idf': 0.069628799451512, 'text': 0.07682709454675062, 'biographi': 0.14036982221366143, 'beatl': 0.14036982221366143, '.': 0.0011444661189109516}, 'Bag of Words is': {'bag': 0.11877446495002121, 'word': 0.033086514772572226, 'effect': 0.11877446495002121, 'model': 0.11877446495002121, 'demonstr': 0.11877446495002121, 'document': 0.045371194993150066, 'numer': 0.09561831143740727, 'vector': 0.11877446495002121, ',': 0.02830590656112265, 'enough': 0.11877446495002121, 'go': 0.07246215792479334, 'enumer': 0.11877446495002121, '.': 0.0009683944083092667}, 'TF-IDF is a tec': {'tf-idf': 0.09573959924582899, 'techniqu': 0.19300850554378446, 'measur': 0.11775100662778916, 'import': 0.1553797560857868, 'word': 0.05376558650542986, 'given': 0.13336834870382663, 'document': 0.07372819186386885, '.': 0.0015736409135025584}, 'TF (Term Freque': {'tf': 0.09420080530223134, '(': 0.10669467896306131, 'term': 0.10669467896306131, 'frequenc': 0.18840161060446267, ')': 0.10669467896306131, 'measur': 0.09420080530223134, 'word': 0.043012469204343894, 'document': 0.05898255349109508, '.': 0.0012589127308020468}, 'IDF (Inverse Do': {'idf': 0.08878843204902102, '(': 0.07621048497361521, 'invers': 0.11029057459644825, 'document': 0.042130395350782196, 'frequenc': 0.06728628950159381, ')': 0.07621048497361521, 'measur': 0.06728628950159381, 'rank': 0.08878843204902102, 'specif': 0.11029057459644825, 'word': 0.030723192288817064, 'relev': 0.11029057459644825, 'within': 0.11029057459644825, 'text': 0.06036414571530405, '.': 0.000899223379144319}, 'Stop words whic': {'stop': 0.05927482164614517, 'word': 0.023895816224635495, 'contain': 0.0690576693714608, 'unnecessari': 0.08578155801945976, 'inform': 0.05233378072346184, '“': 0.20717300811438238, '”': 0.20717300811438238, ',': 0.020443154738588577, 'carri': 0.08578155801945976, 'less': 0.08578155801945976, 'import': 0.0690576693714608, 'spite': 0.08578155801945976, 'occurr': 0.08578155801945976, '.': 0.0006993959615566926}, 'Thus, the TF-ID': {'thu': 0.09082753202060445, ',': 0.0432913865052464, 'tf-idf': 0.0901078581137214, 'product': 0.09082753202060445, 'tf': 0.05541223841307725, 'idf': 0.07311988521684085, ':': 0.07311988521684085, 'order': 0.07311988521684085, 'acquir': 0.07311988521684085, 'good': 0.09082753202060445, 'result': 0.07311988521684085, 'huge': 0.09082753202060445, 'corpu': 0.07311988521684085, 'necessari': 0.07311988521684085, '.': 0.0007405369004717922}, 'In my example, ': {'exampl': 0.17757686409804205, ',': 0.05256811218494206, 'use': 0.1207282914306081, 'small': 0.2205811491928965, 'size': 0.2205811491928965, 'corpu': 0.17757686409804205, '.': 0.001798446758288638}, 'Since I removed': {'sinc': 0.17156311603891952, 'remov': 0.11854964329229034, 'stop': 0.11854964329229034, 'word': 0.04779163244927099, ',': 0.040886309477177155, 'result': 0.1381153387429216, 'wa': 0.17156311603891952, 'pleasant': 0.17156311603891952, '.': 0.0013987919231133851}, 'As my previous ': {'previou': 0.12430380486862945, 'code': 0.12430380486862945, 'piec': 0.15440680443502758, ',': 0.03679767852945944, 'start': 0.15440680443502758, 'ad': 0.15440680443502758, 'modul': 0.15440680443502758, 'use': 0.08450980400142569, 'method': 0.12430380486862945, '.': 0.0012589127308020468}, 'In this example': {'thi': 0.12867233702918962, 'exampl': 0.10358650405719119, ',': 0.06132946421576574, 'util': 0.10358650405719119, 'scikit-learn': 0.08891223246921776, 'besid': 0.12867233702918962, 'numpi': 0.12867233702918962, 'panda': 0.12867233702918962, 'regular': 0.10358650405719119, 'express': 0.10358650405719119, '.': 0.0010490939423350389}, 'Scikit-learn is': {'scikit-learn': 0.15242096994723042, 'free': 0.2205811491928965, 'machin': 0.2205811491928965, 'learn': 0.2205811491928965, 'librari': 0.2205811491928965, 'python': 0.2205811491928965, '.': 0.001798446758288638}, 'We will utilize': {'util': 0.12430380486862945, 'countvector': 0.12430380486862945, 'convert': 0.15440680443502758, 'collect': 0.15440680443502758, 'text': 0.08450980400142569, 'document': 0.05898255349109508, 'matrix': 0.10669467896306131, 'token': 0.15440680443502758, 'count': 0.10669467896306131, '.': 0.0012589127308020468}, 'TfidfTransforme': {'tfidftransform': 0.15440680443502758, 'handl': 0.15440680443502758, 'transform': 0.09420080530223134, 'count': 0.10669467896306131, 'matrix': 0.10669467896306131, 'normal': 0.12430380486862945, 'tf': 0.09420080530223134, 'tf-idf': 0.0765916793966632, 'represent': 0.12430380486862945, '.': 0.0012589127308020468}, 'Data is fetched': {'data': 0.12867233702918962, 'fetch': 0.12867233702918962, '‘': 0.10358650405719119, 'beatles_biographi': 0.12867233702918962, '’': 0.10358650405719119, 'file': 0.12867233702918962, 'pars': 0.12867233702918962, 'text': 0.07042483666785473, 'order': 0.10358650405719119, 'obtain': 0.12867233702918962, 'sentenc': 0.08891223246921776, '.': 0.0010490939423350389}, 'Regular express': {'regular': 0.10358650405719119, 'express': 0.10358650405719119, 'help': 0.12867233702918962, 'separ': 0.12867233702918962, 'sentenc': 0.26673669740765327, 'use': 0.07042483666785473, 'mark': 0.12867233702918962, 'enlist': 0.12867233702918962, 'object': 0.12867233702918962, '.': 0.0010490939423350389}, 'We use 4 parame': {'use': 0.14084967333570947, '4': 0.25734467405837924, 'paramet': 0.15700134217038553, 'countvector': 0.20717300811438238, 'method': 0.20717300811438238, '.': 0.0020981878846700777}, 'First one is st': {'first': 0.14036982221366143, 'one': 0.14036982221366143, 'stop_word': 0.14036982221366143, 'remov': 0.0969951626936921, 'word': 0.039102244731221716, 'occur': 0.14036982221366143, 'lot': 0.14036982221366143, 'contain': 0.11300345897148131, 'necessari': 0.11300345897148131, 'inform': 0.08563709572930121, '.': 0.0011444661189109516}, '‘None’ can be g': {'‘': 0.07311988521684085, 'none': 0.09082753202060445, '’': 0.1462397704336817, 'given': 0.0627615758606243, 'want': 0.07311988521684085, 'remov': 0.0627615758606243, 'ani': 0.09082753202060445, 'word': 0.05060290494628693, 'give': 0.09082753202060445, 'list': 0.0627615758606243, 'choos': 0.09082753202060445, 'go': 0.05541223841307725, 'swept': 0.09082753202060445, 'ourselv': 0.09082753202060445, '.': 0.0007405369004717922}, 'In Scikit-learn': {'scikit-learn': 0.11854964329229034, ',': 0.040886309477177155, 'english': 0.17156311603891952, 'stop': 0.11854964329229034, 'word': 0.04779163244927099, 'list': 0.11854964329229034, 'provid': 0.1381153387429216, 'built-in': 0.17156311603891952, '.': 0.0013987919231133851}, 'min_df paramete': {'min_df': 0.22600691794296263, 'paramet': 0.08563709572930121, 'threshold': 0.14036982221366143, 'valu': 0.0969951626936921, 'ignor': 0.11300345897148131, 'term': 0.0969951626936921, 'document': 0.0536205031737228, 'frequenc': 0.08563709572930121, 'lower': 0.14036982221366143, '.': 0.0011444661189109516}, 'max_df is the c': {'max_df': 0.2486076097372589, 'contrast': 0.30881360887005516, 'min_df': 0.2486076097372589, 'paramet': 0.18840161060446267, '.': 0.0025178254616040935}, 'If the document': {'document': 0.08426079070156439, 'frequenc': 0.13457257900318761, 'word': 0.06144638457763413, 'max_df': 0.17757686409804205, ',': 0.05256811218494206, 'ignor': 0.17757686409804205, '.': 0.001798446758288638}, 'ngram_range(x,y': {'ngram_rang': 0.11029057459644825, '(': 0.07621048497361521, 'x': 0.08878843204902102, ',': 0.02628405609247103, ')': 0.07621048497361521, 'last': 0.11029057459644825, 'paramet': 0.06728628950159381, 'defin': 0.11029057459644825, 'boundari': 0.11029057459644825, 'n': 0.08878843204902102, 'valu': 0.07621048497361521, 'differ': 0.11029057459644825, 'n-gram': 0.08878843204902102, '.': 0.000899223379144319}, 'x is for the mi': {'x': 0.11300345897148131, 'minimum': 0.14036982221366143, 'n': 0.22600691794296263, 'valu': 0.1939903253873842, ',': 0.033452435026781316, 'repres': 0.14036982221366143, 'maximum': 0.14036982221366143, 'n-gram': 0.11300345897148131, '.': 0.0011444661189109516}, 'fit_transform r': {'fit_transform': 0.25734467405837924, 'return': 0.25734467405837924, 'transform': 0.15700134217038553, 'version': 0.25734467405837924, 'sentenc': 0.1778244649384355, '.': 0.0020981878846700777}, 'We transform a ': {'transform': 0.09420080530223134, 'count': 0.10669467896306131, 'matrix': 0.10669467896306131, 'normal': 0.12430380486862945, 'tf': 0.09420080530223134, 'tf-idf': 0.0765916793966632, 'represent': 0.12430380486862945, 'measur': 0.09420080530223134, 'weight': 0.12430380486862945, '.': 0.0012589127308020468}, 'As I mentioned ': {'mention': 0.11300345897148131, 'abov': 0.14036982221366143, ',': 0.033452435026781316, 'word': 0.039102244731221716, 'ha': 0.14036982221366143, 'highest': 0.14036982221366143, 'weight': 0.11300345897148131, 'provid': 0.11300345897148131, 'inform': 0.08563709572930121, 'document': 0.0536205031737228, '.': 0.0011444661189109516}, 'At the end of t': {'end': 0.17156311603891952, 'transform': 0.10466756144692368, ',': 0.040886309477177155, 'list': 0.11854964329229034, 'acquir': 0.1381153387429216, 'compris': 0.17156311603891952, 'term': 0.11854964329229034, 'rank': 0.1381153387429216, '.': 0.0013987919231133851}, 'Finally, we can': {'final': 0.17156311603891952, ',': 0.040886309477177155, 'print': 0.17156311603891952, 'top': 0.17156311603891952, '10': 0.17156311603891952, 'word': 0.04779163244927099, 'given': 0.11854964329229034, 'document': 0.0655361705456612, '.': 0.0013987919231133851}, 'TF-IDF is a num': {'tf-idf': 0.08510186599629244, 'numer': 0.1381153387429216, 'statist': 0.17156311603891952, 'use': 0.09389978222380631, 'inform': 0.10466756144692368, 'retriev': 0.17156311603891952, 'text': 0.09389978222380631, 'mine': 0.17156311603891952, '.': 0.0013987919231133851}, 'I wanted share ': {'want': 0.3107595121715736, 'share': 0.3860170110875689, 'experi': 0.3860170110875689, '.': 0.0031472818270051168}, 'You can find fu': {'find': 0.30881360887005516, 'full': 0.30881360887005516, 'code': 0.2486076097372589, 'repositori': 0.30881360887005516, '.': 0.0025178254616040935}, 'In our next art': {'next': 0.15440680443502758, 'articl': 0.15440680443502758, 'go': 0.09420080530223134, 'continu': 0.15440680443502758, 'implement': 0.12430380486862945, 'word2vec': 0.15440680443502758, 'make': 0.15440680443502758, 'relationship': 0.15440680443502758, 'word': 0.043012469204343894, '.': 0.0012589127308020468}, 'To say me “hi” ': {'say': 0.06433616851459481, '“': 0.051793252028595596, 'hi': 0.06433616851459481, '”': 0.051793252028595596, 'ask': 0.06433616851459481, 'anyth': 0.06433616851459481, ':': 0.3107595121715736, 'e-mail': 0.06433616851459481, 'toprakucar': 0.06433616851459481, '@': 0.06433616851459481, 'gmail.com': 0.06433616851459481, 'linkedin': 0.06433616851459481, 'http': 0.12867233702918962, '//www.linkedin.com/in/ktoprakucar/': 0.06433616851459481, 'github': 0.06433616851459481, '//github.com/ktoprakucar': 0.06433616851459481, 'written': 0.12867233702918962}}\n"
     ]
    }
   ],
   "source": [
    "def create_tf_idf_matrix(tf_matrix, idf_matrix):\n",
    "    tf_idf_matrix = {}\n",
    "    \n",
    "    for (sentence1, freq_table1), (sentence2, freq_table2) in zip(tf_matrix.items(), idf_matrix.items()):\n",
    "        \n",
    "        tf_idf_table = {}\n",
    "        \n",
    "        for (word1, value1), (word2, value2) in zip(freq_table1.items(), freq_table2.items()):\n",
    "            tf_idf_table[word1] = float(value1 * value2)\n",
    "            \n",
    "        tf_idf_matrix[sentence1] = tf_idf_table\n",
    "        \n",
    "    return tf_idf_matrix\n",
    "\n",
    "tf_idf_matrix = create_tf_idf_matrix(tf_matrix, idf_matrix)\n",
    "print(tf_idf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'As I have menti': 0.09334633949351677, 'Bag of Words is': 0.08517182574980796, 'TF-IDF is a tec': 0.10303932943622715, 'TF (Term Freque': 0.08890457705826113, 'IDF (Inverse Do': 0.07427497629773576, 'Stop words whic': 0.08128686531166951, 'Thus, the TF-ID': 0.07098008963552137, 'In my example, ': 0.13877298242224514, 'Since I removed': 0.10888674525486915, 'As my previous ': 0.11131050276076856, 'In this example': 0.09821146863389468, 'Scikit-learn is': 0.17958930895285732, 'We will utilize': 0.10699686511917869, 'TfidfTransforme': 0.10370627792653646, 'Data is fetched': 0.10359830811884323, 'Regular express': 0.11887453212781736, 'We use 4 parame': 0.1619399822796515, 'First one is st': 0.1046122725713087, '‘None’ can be g': 0.0754990093888466, 'In Scikit-learn': 0.10299635939413258, 'min_df paramete': 0.10397795074803873, 'max_df is the c': 0.19938965288212795, 'If the document': 0.098542863060243, 'ngram_range(x,y': 0.08149422807309707, 'x is for the mi': 0.12241228100666511, 'fit_transform r': 0.18482633619477148, 'We transform a ': 0.09467537805661702, 'As I mentioned ': 0.08846150803048784, 'At the end of t': 0.11148987322171965, 'Finally, we can': 0.1067127790936879, 'TF-IDF is a num': 0.11464138563040246, 'I wanted share ': 0.27148520404342913, 'You can find fu': 0.23551325236180568, 'In our next art': 0.1189216818716172, 'To say me “hi” ': 0.08492498308601658}\n"
     ]
    }
   ],
   "source": [
    "def score_sentences(tf_idf_matrix) -> dict:\n",
    "    sentenceValue = {}\n",
    "    \n",
    "    for sentence, score_table in tf_idf_matrix.items():\n",
    "        total_score = 0\n",
    "        \n",
    "        diff_words_in_sentence = len(score_table)\n",
    "        # total_num_wrods_in_sentence = sum([freq_table[t] for t in freq_table])\n",
    "        \n",
    "        \n",
    "        for word, score in score_table.items():\n",
    "            total_score += score\n",
    "            \n",
    "        sentenceValue[sentence] = total_score / diff_words_in_sentence\n",
    "        \n",
    "    return sentenceValue;\n",
    "\n",
    "sentenceScores = score_sentences(tf_idf_matrix)\n",
    "print(sentenceScores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating threshold value used for choosing important sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11798479929412617\n"
     ]
    }
   ],
   "source": [
    "def find_average_score(sentenceScores) -> int:\n",
    "    sumScores = sum([sentenceScores[entry] for entry in sentenceScores])\n",
    "    average = (sumScores / len(sentenceScores))\n",
    "    \n",
    "    return average\n",
    "\n",
    "threshold = find_average_score(sentenceScores)        \n",
    "print(threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In my example, I just used a small sized corpus. Scikit-learn is a free machine learning library for python. Regular expression helps separation of sentences using marks then sentences are enlisted under sentences object. We use 4 parameters in CountVectorizer method. max_df is the contrast of min_df parameter. x is for the minimum n value, y represents the maximum n value for n-grams. fit_transform returns transform version of sentences. I wanted share my experience on it. You can find full code from my repository. In our next article we are going to continue with implementing word2vec to make relationships between words.\n"
     ]
    }
   ],
   "source": [
    "def generate_summary(sentences, sentenceScores, threshold):\n",
    "    sentence_count = 0\n",
    "    summary = ''\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if sentence[:15] in sentenceScores and sentenceScores[sentence[:15]] >= (threshold):\n",
    "            summary += \" \" + sentence\n",
    "            sentence_count += 1\n",
    "            \n",
    "    return summary\n",
    "\n",
    "summary = generate_summary(text, sentenceScores, threshold)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:\n",
      "['As I have mentioned on my previous post, I am going to implement TF-IDF of a text which is a biography of the Beatles.', 'Bag of Words is an effective model to demonstrate documents as numerical vectors, but it is not enough to go further than enumeration.', 'TF-IDF is a technique that measures how important a word in a given document.', 'TF (Term Frequency) measures the frequency of a word in a document.', 'IDF (Inverse Document Frequency) measures the rank of the specific word for its relevancy within the text.', 'Stop words which contain unnecessary information such as “a”, “into” and “and” carry less importance in spite of their occurrence.', 'Thus, the TF-IDF is the product of TF and IDF: In order to acquire good results with TF-IDF, a huge corpus is necessary.', 'In my example, I just used a small sized corpus.', 'Since I removed stop words, result was pleasant.', 'As my previous code piece, we start again by adding modules to use their methods.', 'In this example, we utilize Scikit-learn besides Numpy, Pandas and Regular Expression.', 'Scikit-learn is a free machine learning library for python.', 'We will utilize CountVectorizer to convert a collection of text documents to a matrix of token counts.', 'TfidfTransformers handles transformation of a count matrix to a normalized TF or TF-IDF representation.', 'Data is fetched from ‘beatles_biography’ file and we are parse the text in order to obtain sentences.', 'Regular expression helps separation of sentences using marks then sentences are enlisted under sentences object.', 'We use 4 parameters in CountVectorizer method.', 'First one is stop_words which removes words that occur a lot but do not contain necessary information.', '‘None’ can be given if we don’t want to remove any word or we can give a list to choose which words are going to be swept ourselves.', 'In Scikit-learn, English stop word list is provided built-in.'] \n",
      "\n",
      "Summary:\n",
      " In my example, I just used a small sized corpus. Scikit-learn is a free machine learning library for python. Regular expression helps separation of sentences using marks then sentences are enlisted under sentences object. We use 4 parameters in CountVectorizer method. max_df is the contrast of min_df parameter. x is for the minimum n value, y represents the maximum n value for n-grams. fit_transform returns transform version of sentences. I wanted share my experience on it. You can find full code from my repository. In our next article we are going to continue with implementing word2vec to make relationships between words.\n"
     ]
    }
   ],
   "source": [
    "# Get text from file\n",
    "with open(\"testarticle.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "text = sent_tokenize(text)\n",
    "print(\"Text:\")\n",
    "print(text[:20], \"\\n\")\n",
    "total_documents = len(text)\n",
    "\n",
    "# Calculate Frequency Matrix\n",
    "freq_matrix = create_frequency_matrix(text)\n",
    "# print(\"Frequency Matrix:\")\n",
    "# print(list(freq_matrix)[:5], \"\\n\")\n",
    "\n",
    "# Create Term Frequency Matrix\n",
    "tf_matrix = create_tf_matrix(freq_matrix)\n",
    "# print(\"Term Frequency Matrix:\")\n",
    "# print(list(tf_matrix)[:5], \"\\n\")\n",
    "\n",
    "# Create Document counts per word Matrix\n",
    "doc_per_word = create_documents_per_words(freq_matrix)\n",
    "# print(\"Number of documents each word appear in:\")\n",
    "# print(list(doc_per_word)[:5], \"\\n\")\n",
    "\n",
    "# Create Inverse Document Frequency Matrix\n",
    "idf_matrix = create_idf_matrix(freq_matrix, doc_per_word, total_documents)\n",
    "# print(\"IDF Matrix:\")\n",
    "# print(list(idf_matrix)[:5], \"\\n\")\n",
    "\n",
    "# Create TF-IDF matrix\n",
    "tf_idf_matrix = create_tf_idf_matrix(tf_matrix, idf_matrix)\n",
    "# print(\"TF-IDF Matrix:\")\n",
    "# print(list(tf_idf_matrix)[:5], \"\\n\")\n",
    "\n",
    "# Score each sentence\n",
    "sentenceScores = score_sentences(tf_idf_matrix)\n",
    "# print(\"Scores of each sentence:\")\n",
    "# print(list(sentenceScores)[:5], \"\\n\")\n",
    "\n",
    "# Calculate the threshold to select important sentences for summary\n",
    "threshold = find_average_score(sentenceScores)\n",
    "# print(\"threshold:\")\n",
    "# print(threshold, \"\\n\")\n",
    "\n",
    "# Generate the summary\n",
    "summary = generate_summary(text, sentenceScores, threshold)\n",
    "print(\"Summary:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Calculate Frequency Matrix for words Per Sentence(or document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['As I have menti', 'Bag of Words is', 'TF-IDF is a tec', 'TF (Term Freque', 'IDF (Inverse Do', 'Stop words whic', 'Thus, the TF-ID', 'In my example, ', 'Since I removed', 'As my previous ', 'In this example', 'Scikit-learn is', 'We will utilize', 'TfidfTransforme', 'Data is fetched', 'Regular express', 'We use 4 parame', 'First one is st', '‘None’ can be g', 'In Scikit-learn', 'min_df paramete', 'max_df is the c', 'If the document', 'ngram_range(x,y', 'x is for the mi', 'fit_transform r', 'We transform a ', 'As I mentioned ', 'At the end of t', 'Finally, we can', 'TF-IDF is a num', 'I wanted share ', 'You can find fu', 'In our next art', 'To say me “hi” ']\n",
      "{'As I have menti': {'mention': 1, 'previou': 1, 'post': 1, ',': 1, 'go': 1, 'implement': 1, 'tf-idf': 1, 'text': 1, 'biographi': 1, 'beatl': 1, '.': 1}, 'Bag of Words is': {'bag': 1, 'word': 1, 'effect': 1, 'model': 1, 'demonstr': 1, 'document': 1, 'numer': 1, 'vector': 1, ',': 1, 'enough': 1, 'go': 1, 'enumer': 1, '.': 1}, 'TF-IDF is a tec': {'tf-idf': 1, 'techniqu': 1, 'measur': 1, 'import': 1, 'word': 1, 'given': 1, 'document': 1, '.': 1}, 'TF (Term Freque': {'tf': 1, '(': 1, 'term': 1, 'frequenc': 2, ')': 1, 'measur': 1, 'word': 1, 'document': 1, '.': 1}, 'IDF (Inverse Do': {'idf': 1, '(': 1, 'invers': 1, 'document': 1, 'frequenc': 1, ')': 1, 'measur': 1, 'rank': 1, 'specif': 1, 'word': 1, 'relev': 1, 'within': 1, 'text': 1, '.': 1}, 'Stop words whic': {'stop': 1, 'word': 1, 'contain': 1, 'unnecessari': 1, 'inform': 1, '“': 3, '”': 3, ',': 1, 'carri': 1, 'less': 1, 'import': 1, 'spite': 1, 'occurr': 1, '.': 1}, 'Thus, the TF-ID': {'thu': 1, ',': 2, 'tf-idf': 2, 'product': 1, 'tf': 1, 'idf': 1, ':': 1, 'order': 1, 'acquir': 1, 'good': 1, 'result': 1, 'huge': 1, 'corpu': 1, 'necessari': 1, '.': 1}, 'In my example, ': {'exampl': 1, ',': 1, 'use': 1, 'small': 1, 'size': 1, 'corpu': 1, '.': 1}, 'Since I removed': {'sinc': 1, 'remov': 1, 'stop': 1, 'word': 1, ',': 1, 'result': 1, 'wa': 1, 'pleasant': 1, '.': 1}, 'As my previous ': {'previou': 1, 'code': 1, 'piec': 1, ',': 1, 'start': 1, 'ad': 1, 'modul': 1, 'use': 1, 'method': 1, '.': 1}, 'In this example': {'thi': 1, 'exampl': 1, ',': 2, 'util': 1, 'scikit-learn': 1, 'besid': 1, 'numpi': 1, 'panda': 1, 'regular': 1, 'express': 1, '.': 1}, 'Scikit-learn is': {'scikit-learn': 1, 'free': 1, 'machin': 1, 'learn': 1, 'librari': 1, 'python': 1, '.': 1}, 'We will utilize': {'util': 1, 'countvector': 1, 'convert': 1, 'collect': 1, 'text': 1, 'document': 1, 'matrix': 1, 'token': 1, 'count': 1, '.': 1}, 'TfidfTransforme': {'tfidftransform': 1, 'handl': 1, 'transform': 1, 'count': 1, 'matrix': 1, 'normal': 1, 'tf': 1, 'tf-idf': 1, 'represent': 1, '.': 1}, 'Data is fetched': {'data': 1, 'fetch': 1, '‘': 1, 'beatles_biographi': 1, '’': 1, 'file': 1, 'pars': 1, 'text': 1, 'order': 1, 'obtain': 1, 'sentenc': 1, '.': 1}, 'Regular express': {'regular': 1, 'express': 1, 'help': 1, 'separ': 1, 'sentenc': 3, 'use': 1, 'mark': 1, 'enlist': 1, 'object': 1, '.': 1}, 'We use 4 parame': {'use': 1, '4': 1, 'paramet': 1, 'countvector': 1, 'method': 1, '.': 1}, 'First one is st': {'first': 1, 'one': 1, 'stop_word': 1, 'remov': 1, 'word': 1, 'occur': 1, 'lot': 1, 'contain': 1, 'necessari': 1, 'inform': 1, '.': 1}, '‘None’ can be g': {'‘': 1, 'none': 1, '’': 2, 'given': 1, 'want': 1, 'remov': 1, 'ani': 1, 'word': 2, 'give': 1, 'list': 1, 'choos': 1, 'go': 1, 'swept': 1, 'ourselv': 1, '.': 1}, 'In Scikit-learn': {'scikit-learn': 1, ',': 1, 'english': 1, 'stop': 1, 'word': 1, 'list': 1, 'provid': 1, 'built-in': 1, '.': 1}, 'min_df paramete': {'min_df': 2, 'paramet': 1, 'threshold': 1, 'valu': 1, 'ignor': 1, 'term': 1, 'document': 1, 'frequenc': 1, 'lower': 1, '.': 1}, 'max_df is the c': {'max_df': 1, 'contrast': 1, 'min_df': 1, 'paramet': 1, '.': 1}, 'If the document': {'document': 1, 'frequenc': 1, 'word': 1, 'max_df': 1, ',': 1, 'ignor': 1, '.': 1}, 'ngram_range(x,y': {'ngram_rang': 1, '(': 1, 'x': 1, ',': 1, ')': 1, 'last': 1, 'paramet': 1, 'defin': 1, 'boundari': 1, 'n': 1, 'valu': 1, 'differ': 1, 'n-gram': 1, '.': 1}, 'x is for the mi': {'x': 1, 'minimum': 1, 'n': 2, 'valu': 2, ',': 1, 'repres': 1, 'maximum': 1, 'n-gram': 1, '.': 1}, 'fit_transform r': {'fit_transform': 1, 'return': 1, 'transform': 1, 'version': 1, 'sentenc': 1, '.': 1}, 'We transform a ': {'transform': 1, 'count': 1, 'matrix': 1, 'normal': 1, 'tf': 1, 'tf-idf': 1, 'represent': 1, 'measur': 1, 'weight': 1, '.': 1}, 'As I mentioned ': {'mention': 1, 'abov': 1, ',': 1, 'word': 1, 'ha': 1, 'highest': 1, 'weight': 1, 'provid': 1, 'inform': 1, 'document': 1, '.': 1}, 'At the end of t': {'end': 1, 'transform': 1, ',': 1, 'list': 1, 'acquir': 1, 'compris': 1, 'term': 1, 'rank': 1, '.': 1}, 'Finally, we can': {'final': 1, ',': 1, 'print': 1, 'top': 1, '10': 1, 'word': 1, 'given': 1, 'document': 1, '.': 1}, 'TF-IDF is a num': {'tf-idf': 1, 'numer': 1, 'statist': 1, 'use': 1, 'inform': 1, 'retriev': 1, 'text': 1, 'mine': 1, '.': 1}, 'I wanted share ': {'want': 1, 'share': 1, 'experi': 1, '.': 1}, 'You can find fu': {'find': 1, 'full': 1, 'code': 1, 'repositori': 1, '.': 1}, 'In our next art': {'next': 1, 'articl': 1, 'go': 1, 'continu': 1, 'implement': 1, 'word2vec': 1, 'make': 1, 'relationship': 1, 'word': 1, '.': 1}, 'To say me “hi” ': {'say': 1, '“': 1, 'hi': 1, '”': 1, 'ask': 1, 'anyth': 1, ':': 6, 'e-mail': 1, 'toprakucar': 1, '@': 1, 'gmail.com': 1, 'linkedin': 1, 'http': 2, '//www.linkedin.com/in/ktoprakucar/': 1, 'github': 1, '//github.com/ktoprakucar': 1, 'written': 2}}\n"
     ]
    }
   ],
   "source": [
    "#this function should iterate through each sentence and save the frequency of each word in the sentence\n",
    "\n",
    "def create_frequency_matrix(sentences) -> dict:\n",
    "    # set stopwords and stemmer\n",
    "    stopWords = set(stopwords.words(\"english\"))\n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    #initialize freqency matrix\n",
    "    frequency_matrix = {}\n",
    "    keys = []\n",
    "    value_table = []\n",
    "    \n",
    "    # get first 15 chars per sentence as key for frequency matrix\n",
    "    keys = list(map(lambda x: x[:15], sentences))\n",
    "    #print(keys)\n",
    "    \n",
    "    #create list of dicts for word frequency for each sentence\n",
    "    for sentence in sentences:\n",
    "        freq_table = {}\n",
    "        # tokenize words: SPLITTING\n",
    "        tokenized = word_tokenize(sentence)\n",
    "        # clean words: REDUCING\n",
    "        for word in tokenized:\n",
    "            word = word.lower()\n",
    "            word = ps.stem(word)\n",
    "            if word in stopWords:\n",
    "                continue\n",
    "            if word in freq_table:\n",
    "                freq_table[word] += 1\n",
    "            else:\n",
    "                freq_table[word] = 1\n",
    "\n",
    "        value_table.append(freq_table)\n",
    "        #print(freq_table)\n",
    "    return dict(zip(keys, value_table))\n",
    "\n",
    "# open file\n",
    "with open(\"testarticle.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# tokenize article text into sentence and save as a list.\n",
    "# MAP REDUCE SPLITTING PHASE\n",
    "text = sent_tokenize(text)\n",
    "\n",
    "frequency_matrix = create_frequency_matrix(text)\n",
    "print(frequency_matrix)\n",
    "#print(mapping)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tldr bot",
   "language": "python",
   "name": "tldr_bot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
